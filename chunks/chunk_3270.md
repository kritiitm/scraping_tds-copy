### Post 206
**Post URL**: /t/tds-official-project1-discrepencies/171141/206
- **ID**: 614374
- **Author**: Shreyan Chaubey (thinkmachine)
- **Created At**: 2025-04-01T19:02:11.386Z
- **Reply To**: Post 192 (Jivraj Singh Shekhawat, Jivraj)
- **Content**:  
  <a class="mention" href="/u/jivraj">@Jivraj</a> <a class="mention" href="/u/carlton">@Carlton</a> I completely understand that changes to the Docker image after the deadline cannot be accepted.
<em><strong>However, there are specific cases like mine where the Project 1 submission successfully passed the sanity checks on Feb 15 and received a decent score when the evaluation results were released on Mar 29.</strong></em>
<div class="lightbox-wrapper"><a class="lightbox" href="https://europe1.discourse-cdn.com/flex013/uploads/iitm/original/3X/9/a/9acc2fc47ea84b9e26c1ed21442ba873d0dca20e.png" data-download-href="/uploads/short-url/m5oZT6ccNhYAMcg96GqKgDnNOWO.png?dl=1" title="image" rel="noopener nofollow ugc"><img src="https://europe1.discourse-cdn.com/flex013/uploads/iitm/original/3X/9/a/9acc2fc47ea84b9e26c1ed21442ba873d0dca20e.png" alt="image" data-base62-sha1="m5oZT6ccNhYAMcg96GqKgDnNOWO" width="690" height="214" data-dominant-color="EEEFF1"><div class="meta"><svg class="fa d-icon d-icon-far-image svg-icon" aria-hidden="true"><use href="#far-image"></use></svg><span class="filename">image</span><span class="informations">1272×395 25.7 KB</span><svg class="fa d-icon d-icon-discourse-expand svg-icon" aria-hidden="true"><use href="#discourse-expand"></use></svg></div></a></div>
But here’s the catch:** Since the problem statement for Project 1 and Project 2 is nearly the same, I took the opportunity to improve upon my Project 1 and use it as the foundation for my Project 2 submission, which I did by:*
<ul>
<li>Implementing a ReAct-based workflow planning &amp; orchestration agent, inspired by the paper <a href="https://arxiv.org/abs/2210.03629" rel="noopener nofollow ugc">ReAct: Synergizing Reasoning and Acting in Language Models</a>.</li>
<li>Implementing various tools for web-serping, web-scraping, read-eval-print-loops interpreters for quick calculations, etc.</li>
<li>Enhancing Shell-use &amp; Python-use by improving upon the existing code interpreter I had implemented for P1. This allowed the agent to dynamically generate and execute code without hardcoding anything.</li>
<li>Adding useful API endpoints, including an <strong><code>/api/</code></strong> multipart/form endpoint, alongside the existing <strong><code>/read</code></strong> and <strong><code>/run</code></strong> endpoints from Project 1, plus a <strong><code>/clear</code></strong> endpoint to reset the agent’s conversation memory if the context window gets saturated.</li>
<li><strong>Deploying the entire project on a paid GCP VM Instance with a static IP</strong>, utilizing my own OpenAI API key while keeping OpenAI’s API as a fallback in case AIPROXY ever gave up.</li>
</ul>
All this hard work evolved my project into something far beyond a simple Tool-Calling Agent—it essentially became a ReAct Principles based Computer-Using Agent capable of executing complex, non-linear workflows entirely within a container. And I’m not exaggerating: You could ask it to perform something like <strong>hyperparameter tuning for a Random Forest Classifier, offloading the results locally on a JSON file and displaying its contents</strong>, and it would do that for you—without <strong>ever</strong> declining the request. I like to think of it as a <strong>terminal version of</strong> <a href="https://openai.com/index/computer-using-agent/" rel="noopener nofollow ugc">OpenAI’s Computer-Using Agent</a>.
<hr>
Given all the effort, time, and money that went into this, it’s incredibly discouraging to see my project <strong>naturally fail a sanity check (Docker image digest mismatch) (because of the aforesaid updates)</strong> and not get evaluated as a result. This is not the kind of experience that encourages students to learn, experiment, and innovate.
<h2><a name="p-614374-to-clarify-all-the-updates-mentioned-above-took-place-after-march-29-after-project-1-had-already-been-evaluated-and-results-had-been-handed-out-furthermore-we-were-never-informed-that-a-reevaluation-would-take-place-on-april-1-had-i-known-i-would-have-ensured-that-my-original-submission-remained-unchanged-and-considered-creating-a-duplicate-of-my-docker-image-and-implementing-all-the-aforementioned-enhancements-on-it-1" class="anchor" href="#p-614374-to-clarify-all-the-updates-mentioned-above-took-place-after-march-29-after-project-1-had-already-been-evaluated-and-results-had-been-handed-out-furthermore-we-were-never-informed-that-a-reevaluation-would-take-place-on-april-1-had-i-known-i-would-have-ensured-that-my-original-submission-remained-unchanged-and-considered-creating-a-duplicate-of-my-docker-image-and-implementing-all-the-aforementioned-enhancements-on-it-1"></a>To clarify, <strong>all the updates mentioned above took place after March 29</strong>, <strong>after Project 1 had already been evaluated, and results had been handed out.</strong> Furthermore, we were <strong>never informed</strong> that a reevaluation would take place on April 1. Had I known, I would have ensured that my original submission remained unchanged and considered creating a duplicate of my Docker image and implementing all the aforementioned enhancements on it.</h2>
My only request is that if my <strong>updated P1 submission cannot be evaluated</strong> due to the changes made after March 29 (before the P1 reevaluation on April 1), I would really appreciate it if my <strong>original P1 eval score could be reinstated</strong> instead of receiving a <strong>0</strong>—since it was already evaluated and graded.
Would highly appreciate your prompt support in this regard <a class="mention" href="/u/carlton">@carlton</a> <a class="mention" href="/u/jivraj">@Jivraj</a>
- **Reactions**: None
- **Post Number**: 206

